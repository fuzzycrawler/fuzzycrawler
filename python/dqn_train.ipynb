{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, Input, Lambda\n",
    "from tensorflow.keras.layers import  RepeatVector, TimeDistributed, GlobalMaxPooling1D, Embedding, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Grammar1 to Grammar5 files stored in all_grammar_inputs.txt\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from Stub.Stub import StubSession\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from grammar_lib.testCaseModifier import GCheckModifier\n",
    "from grammar_lib.SQLChecker import parser\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "with open(\"grammar_lib/all_grammar_inputs.txt\") as file:\n",
    "    all_grammar_inputs = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "all_grammar_inputs_mod = []\n",
    "for gram_inp in all_grammar_inputs:\n",
    "    if not ('UNION' in gram_inp or 'ORDER' in gram_inp or 'admin' in gram_inp.lower()):\n",
    "        all_grammar_inputs_mod.append(gram_inp)\n",
    "        all_grammar_inputs_mod.append(gram_inp.replace('OR', 'AND'))\n",
    "\n",
    "all_grammar_inputs = all_grammar_inputs_mod.copy()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sent = all_grammar_inputs.copy()\n",
    "\n",
    "sent_processed = []\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "for input_str in sent:\n",
    "    input_str = str(input_str)\n",
    "    input_str = input_str.lower()\n",
    "    input_str = input_str.strip()\n",
    "    sent_processed.append(nltk.word_tokenize(input_str))\n",
    "    # sent_processed.append(tokenizer.tokenize(input_str))\n",
    "    \n",
    "all_vocab = [v_w for v in sent_processed for v_w in v]\n",
    "\n",
    "v_count = dict(Counter(all_vocab))\n",
    "v_count = dict(sorted(v_count.items(), key=lambda item: item[1], reverse=True))\n",
    "all_vocab = list(v_count.keys())\n",
    "\n",
    "all_vocab.sort()\n",
    "\n",
    "ind_list = list(range(1, len(all_vocab)+1))\n",
    "\n",
    "word2ind = dict(zip(all_vocab, ind_list))\n",
    "ind2word = dict(zip(ind_list, all_vocab))\n",
    "\n",
    "word2ind['<EOS>'] = 0\n",
    "ind2word[0] = '<EOS>'\n",
    "\n",
    "all_vocab = ['<EOS>']+all_vocab\n",
    "\n",
    "VOCAB_SIZE = len(all_vocab)\n",
    "\n",
    "VOCAB_SIZE\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# gmod = GCheckModifier()\n",
    "# gparse = parser()\n",
    "\n",
    "# str_test = \"('  UNION select pass email from DUAL #\"#all_grammar_inputs[1000]\n",
    "# print(str_test)\n",
    "# for_parsing = gmod.grammarchecker(str_test) \n",
    "# print(for_parsing)\n",
    "# gparse.main(for_parsing) # 1 for failed\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 10_000  # last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 150 \n",
    "MINIBATCH_SIZE = 128\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MODEL_NAME = 'RLFuzzv1.1'\n",
    "MIN_REWARD = -200\n",
    "MAX_LENGTH = 11 # including EOS\n",
    "\n",
    "EPISODES = 4_000\n",
    "\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.999\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 50  # episodes\n",
    "\n",
    "# succ = [word2ind[s] for s in \"( ' OR 1 = 1 ; -- )\".lower().split()]+[0, 0] # maintain max_length & EOS\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "loaded_test = [] # store compatible length grammar based init (generation) test strings\n",
    "for sent in sent_processed:\n",
    "    if len(sent)<=MAX_LENGTH-1:\n",
    "        loaded_test.append(sent)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def eos_and_ind(sampled_list: list, ind: bool = False):\n",
    "    if ind:\n",
    "        eos_token = 0\n",
    "    else:\n",
    "        eos_token = \"<EOS>\"\n",
    "    clip_ind = sampled_list.index(eos_token)\n",
    "    if clip_ind < MAX_LENGTH-1:\n",
    "        clip_ind_rem = MAX_LENGTH-clip_ind\n",
    "        sampled_list = sampled_list[:clip_ind]+[eos_token]*clip_ind_rem # slower than if\n",
    "    assert len(sampled_list) == MAX_LENGTH\n",
    "    if ind:\n",
    "        return sampled_list\n",
    "    return [word2ind[s] for s in sampled_list]\n",
    "\n",
    "def init_string_list():\n",
    "    gram_gen_str = loaded_test[random.randint(0, len(loaded_test)-1)] # randint a<=N<=b\n",
    "    sampled_list = gram_gen_str+(MAX_LENGTH-len(gram_gen_str))*['<EOS>']\n",
    "    # sampled_list = list(random.sample(all_vocab, MAX_LENGTH-1))+['<EOS>']\n",
    "    return eos_and_ind(sampled_list)\n",
    "\n",
    "def mutate_string_list(seed: list, pos: int = None, vocab: int = None):\n",
    "    if vocab is None:\n",
    "        vocab_str = random.sample(all_vocab, 1)[0]\n",
    "        vocab = word2ind[vocab_str]\n",
    "    if pos is None:\n",
    "        pos = random.randint(0, MAX_LENGTH-2) # MAX_LENGTH-2 inclusive\n",
    "    if pos != MAX_LENGTH-1: # should never replace EOS\n",
    "        seed[pos] = vocab\n",
    "    return eos_and_ind(seed, ind=True)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class RLFuzz:\n",
    "    def __init__(self, rewarding=None):\n",
    "        if rewarding is None:\n",
    "            self.init_string = init_string_list() # always as index\n",
    "        else:\n",
    "            self.init_string = rewarding.copy()\n",
    "        self.seed_str = self.init_string.copy()\n",
    "        self.last_str = self.init_string.copy()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\\nOrig seed string: \\n{\" \".join([ind2word[s] for s in self.init_string])}\\nLast seed string: \\n{\" \".join([ind2word[s] for s in self.last_str])}\\nCurr seed string: \\n{\" \".join([ind2word[s] for s in self.seed_str])}'\n",
    "\n",
    "    def action(self, pos, vocab):\n",
    "        self.last_str = self.seed_str.copy()\n",
    "        self.seed_str = mutate_string_list(seed=self.seed_str, pos=pos, vocab=vocab)\n",
    "\n",
    "class RLFuzzEnv:\n",
    "    MUTATION_PENALTY = 1\n",
    "    SAME_STRING_PENALTY = 10 # eos & grammar related\n",
    "    PARSER_PENALTY = 20\n",
    "    SUCCESS_REWARD = 1000\n",
    "    ACTION_SPACE_SIZE_POS = MAX_LENGTH\n",
    "    ACTION_SPACE_SIZE_VOCAB = VOCAB_SIZE\n",
    "\n",
    "    def reset(self, rewarding=None):\n",
    "        self.session = StubSession()\n",
    "        self.last_status = 0\n",
    "        self.fuzzer = RLFuzz(rewarding)\n",
    "        self.episode_step = 0\n",
    "        observation = self.fuzzer.init_string\n",
    "        \n",
    "        self.gmod = GCheckModifier()\n",
    "        self.gparse = parser()\n",
    "        \n",
    "        self.operation = 'None'\n",
    "        \n",
    "        return observation\n",
    "\n",
    "    def step(self, action_pos, action_vocab):       \n",
    "        fuzzing_success = False # init  \n",
    "        self.episode_step += 1\n",
    "        self.fuzzer.action(action_pos, action_vocab)\n",
    "        new_observation = self.fuzzer.seed_str\n",
    "        \n",
    "        eos_index = new_observation.index(0)\n",
    "        new_observation_ = new_observation[:eos_index]\n",
    "        username_rl = \" \".join([ind2word[s] for s in new_observation_])\n",
    "                \n",
    "        eos_indexL = self.fuzzer.last_str.index(0)\n",
    "        new_observationL_ = self.fuzzer.last_str[:eos_indexL]\n",
    "        last_username_rl = \" \".join([ind2word[s] for s in new_observationL_])\n",
    "                \n",
    "        parser_failed = self.gparse.main(self.gmod.grammarchecker(username_rl))\n",
    "        \n",
    "        if len(username_rl.strip()) == 0 or last_username_rl==username_rl: # rudimentary\n",
    "#             print(f\"SAME_STRING_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "            reward = -self.SAME_STRING_PENALTY\n",
    "        elif parser_failed: # parser\n",
    "#             print(f\"PARSER_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "            reward = -self.PARSER_PENALTY\n",
    "        else: # check via website\n",
    "            if self.last_status == 1:\n",
    "                self.session.reset_session()\n",
    "\n",
    "            url=\"http://localhost/demo/example_mysql_injection_login.php\"\n",
    "            jsonFilePath = './Stub/conditions.json'\n",
    "            receive=self.session.s.get(url)\n",
    "            form_details,keys=self.session.preprocessing_Form_Fields(url)\n",
    "\n",
    "            values=[username_rl, \"RaNdOmStRiNg\"]\n",
    "            logindata=self.session.form_input_feeding(keys,values,form_details)\n",
    "            pass_Conditions, fail_Conditions = self.session.jsonReading(jsonFilePath)\n",
    "            status = self.session.validation(url, logindata, keys, pass_Conditions, fail_Conditions)\n",
    "            self.last_status = status\n",
    "\n",
    "            fuzzing_success = True if status==1 else False\n",
    "\n",
    "            if fuzzing_success:\n",
    "                print(f\"\\nSUCCESS_REWARD @ {self.episode_step}: \")\n",
    "                print(self.fuzzer)\n",
    "                print(f\"OP: {self.operation}, pos: {action_pos}, vocab: {action_vocab}-{ind2word[action_vocab]}\")\n",
    "                reward = self.SUCCESS_REWARD\n",
    "            else:\n",
    "    #             print(f\"MUTATION_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "                reward = -self.MUTATION_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if self.episode_step >= 100 or (fuzzing_success and self.episode_step>25): #TODO: chk -- removed: @ SUCCESS_REWARD\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output, weights = self.att(inputs, inputs, return_attention_scores=True)\n",
    "        attn_output1 = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output1)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output), weights\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "# Agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "        #self.tensorboard = ModifiedTensorBoard(log_dir=\"logs\\\\{}-{}\".format(MODEL_NAME, int(time.time())), profile_batch = 10000000)\n",
    "        #$REM\n",
    "        \n",
    "#     def create_model(self):\n",
    "#         embed_dim = 64  # Embedding size for each token\n",
    "#         num_heads = 4  # Number of attention heads\n",
    "#         ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "#         inputs = Input(shape=(MAX_LENGTH,))\n",
    "#         embedding_layer = TokenAndPositionEmbedding(MAX_LENGTH, VOCAB_SIZE, embed_dim)\n",
    "#         x = embedding_layer(inputs)\n",
    "#         transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "#         x, attn_output = transformer_block(x)\n",
    "#         x = layers.GlobalAveragePooling1D()(x)\n",
    "#         x = layers.Dropout(0.1)(x)\n",
    "#         x = layers.Dense(64, activation=\"relu\")(x)\n",
    "#         x = layers.Dropout(0.1)(x)\n",
    "\n",
    "#         x_pos = layers.Dense(32, activation=\"relu\")(x)\n",
    "#         x_pos = layers.Dropout(0.1)(x_pos)\n",
    "#         x_pos = layers.Dense(env.ACTION_SPACE_SIZE_POS, activation='linear', name='q_pos')(x_pos)\n",
    "\n",
    "#         x_vocab = layers.Dense(64, activation=\"relu\")(x)\n",
    "#         x_vocab = layers.Dropout(0.1)(x_vocab)\n",
    "#         x_vocab = layers.Dense(env.ACTION_SPACE_SIZE_VOCAB, activation='linear', name='q_vocab')(x_vocab)\n",
    "\n",
    "#         model = keras.Model(inputs=inputs, outputs=[x_pos, x_vocab])\n",
    "#         model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=\"accuracy\")\n",
    "#         return model\n",
    "    \n",
    "    def create_model(self):\n",
    "        inputs = Input(shape=(MAX_LENGTH,))\n",
    "\n",
    "        embed=Embedding(VOCAB_SIZE, 100)(inputs)\n",
    "\n",
    "        activations= keras.layers.GRU(250, return_sequences=True)(embed)\n",
    "\n",
    "        attention = TimeDistributed(Dense(1, activation='tanh'))(activations)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(250)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = keras.layers.multiply([activations, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
    "\n",
    "        x_pos = Dense(32, activation=\"relu\")(sent_representation)\n",
    "        x_pos = Dropout(0.1)(x_pos)\n",
    "        x_pos = Dense(env.ACTION_SPACE_SIZE_POS, activation='linear', name='q_pos')(x_pos)\n",
    "\n",
    "        x_vocab = Dense(32, activation=\"relu\")(sent_representation)\n",
    "        x_vocab = Dropout(0.1)(x_vocab)\n",
    "        x_vocab = Dense(env.ACTION_SPACE_SIZE_VOCAB, activation='linear', name='q_vocab')(x_vocab)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=[x_pos, x_vocab])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    # (observation space, action_pos, action_vocab, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self, terminal_state, step):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        # print(current_qs_list[0].shape, current_qs_list[1].shape) # 64,11 & 64,60\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[-2] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X, y_pos, y_vocab = [], [], []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action_pos, action_vocab, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q_pos = np.max(future_qs_list[0][index])\n",
    "                new_q_pos = reward + DISCOUNT * max_future_q_pos\n",
    "                \n",
    "                max_future_q_vocab = np.max(future_qs_list[1][index])\n",
    "                new_q_vocab = reward + DISCOUNT * max_future_q_vocab\n",
    "            else:\n",
    "                new_q_pos = reward\n",
    "                new_q_vocab = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs_pos = current_qs_list[0][index]\n",
    "            current_qs_pos[action_pos] = new_q_pos\n",
    "            \n",
    "            current_qs_vocab = current_qs_list[1][index]\n",
    "            current_qs_vocab[action_vocab] = new_q_vocab\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y_pos.append(current_qs_pos)\n",
    "            y_vocab.append(current_qs_vocab)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X), [np.array(y_pos), np.array(y_vocab)], batch_size=MINIBATCH_SIZE, verbose=0, \n",
    "                       shuffle=False if terminal_state else None)\n",
    "        #$REM: ,callbacks=[self.tensorboard] \n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.expand_dims(np.array(state), axis=0))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "env = RLFuzzEnv()\n",
    "ep_rewards = [-200]\n",
    "\n",
    "# random.seed(1)\n",
    "# np.random.seed(1)\n",
    "# tf.random.set_seed(1)\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode #$REM\n",
    "    # agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward, step number, and env + get current state\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "#     if episode == 2:\n",
    "#         current_state = env.reset(succ)\n",
    "#     else:\n",
    "    current_state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            q_values = agent.get_qs(current_state)\n",
    "            action_pos = np.argmax(q_values[0][0])\n",
    "            action_vocab = np.argmax(q_values[1][0])\n",
    "            env.operation = 'model_argmax'\n",
    "            # action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            if np.random.random() > 0.8:\n",
    "                action_pos = random.sample([0,10], 1)[0]\n",
    "                action_vocab = random.sample([word2ind[\"<EOS>\"],word2ind[\"'\"]], 1)[0]\n",
    "                env.operation = 'random_force'\n",
    "            else:\n",
    "                action_pos = np.random.randint(0, env.ACTION_SPACE_SIZE_POS-1) # randint a<=N<=b\n",
    "                action_vocab = np.random.randint(0, env.ACTION_SPACE_SIZE_VOCAB-1)\n",
    "                env.operation = 'random'\n",
    "\n",
    "        new_state, reward, done = env.step(action_pos, action_vocab)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # At every step update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action_pos, action_vocab, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Logging\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        #$REM agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "        #if min_reward > MIN_REWARD:\n",
    "        #agent.model.save(f'models/{MODEL_NAME}_ep{episode}__epsilon{epsilon:_>7.2f}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-premises",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
