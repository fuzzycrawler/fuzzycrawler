{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "superior-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, Input, Lambda\n",
    "from tensorflow.keras.layers import  RepeatVector, TimeDistributed, GlobalMaxPooling1D, Embedding, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preceding-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grammar1 to Grammar5 files stored in all_grammar_inputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "royal-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Stub.Stub import StubSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imposed-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammar_lib.testCaseModifier import GCheckModifier\n",
    "from grammar_lib.SQLChecker import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "american-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grammar_lib/all_grammar_inputs.txt\") as file:\n",
    "    all_grammar_inputs = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handy-geology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = all_grammar_inputs.copy()\n",
    "\n",
    "sent_processed = []\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "for input_str in sent:\n",
    "    input_str = str(input_str)\n",
    "    input_str = input_str.lower()\n",
    "    input_str = input_str.strip()\n",
    "    sent_processed.append(nltk.word_tokenize(input_str))\n",
    "    # sent_processed.append(tokenizer.tokenize(input_str))\n",
    "    \n",
    "all_vocab = [v_w for v in sent_processed for v_w in v]\n",
    "\n",
    "v_count = dict(Counter(all_vocab))\n",
    "v_count = dict(sorted(v_count.items(), key=lambda item: item[1], reverse=True))\n",
    "all_vocab = list(v_count.keys())\n",
    "\n",
    "all_vocab.sort()\n",
    "\n",
    "ind_list = list(range(1, len(all_vocab)+1))\n",
    "\n",
    "word2ind = dict(zip(all_vocab, ind_list))\n",
    "ind2word = dict(zip(ind_list, all_vocab))\n",
    "\n",
    "word2ind['<EOS>'] = 0\n",
    "ind2word[0] = '<EOS>'\n",
    "\n",
    "all_vocab = ['<EOS>']+all_vocab\n",
    "\n",
    "VOCAB_SIZE = len(all_vocab)\n",
    "\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modular-alert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('  UNION select pass email from DUAL #\n",
      "SELECT * FROM abc where username=def  UNION select pass email from DUAL #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gmod = GCheckModifier()\n",
    "# gparse = parser()\n",
    "\n",
    "# str_test = \"('  UNION select pass email from DUAL #\"#all_grammar_inputs[1000]\n",
    "# print(str_test)\n",
    "# for_parsing = gmod.grammarchecker(str_test) \n",
    "# print(for_parsing)\n",
    "# gparse.main(for_parsing) # 1 for failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wanted-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 5_000  # last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 150 \n",
    "MINIBATCH_SIZE = 128\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MODEL_NAME = 'RLFuzzv1.0'\n",
    "MIN_REWARD = -200\n",
    "MAX_LENGTH = 11 # including EOS\n",
    "\n",
    "EPISODES = 1_000\n",
    "\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 100  # episodes\n",
    "\n",
    "# succ = [word2ind[s] for s in \"( ' OR 1 = 1 ; -- )\".lower().split()]+[0, 0] # maintain max_length & EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "split-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_test = [] # store compatible length grammar based init (generation) test strings\n",
    "for sent in sent_processed:\n",
    "    if len(sent)<=MAX_LENGTH-1:\n",
    "        loaded_test.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loved-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eos_and_ind(sampled_list: list, ind: bool = False):\n",
    "    if ind:\n",
    "        eos_token = 0\n",
    "    else:\n",
    "        eos_token = \"<EOS>\"\n",
    "    clip_ind = sampled_list.index(eos_token)\n",
    "    if clip_ind < MAX_LENGTH-1:\n",
    "        clip_ind_rem = MAX_LENGTH-clip_ind\n",
    "        sampled_list = sampled_list[:clip_ind]+[eos_token]*clip_ind_rem # slower than if\n",
    "    assert len(sampled_list) == MAX_LENGTH\n",
    "    if ind:\n",
    "        return sampled_list\n",
    "    return [word2ind[s] for s in sampled_list]\n",
    "\n",
    "def init_string_list():\n",
    "    gram_gen_str = loaded_test[random.randint(0, len(loaded_test)-1)] # randint a<=N<=b\n",
    "    sampled_list = gram_gen_str+(MAX_LENGTH-len(gram_gen_str))*['<EOS>']\n",
    "    # sampled_list = list(random.sample(all_vocab, MAX_LENGTH-1))+['<EOS>']\n",
    "    return eos_and_ind(sampled_list)\n",
    "\n",
    "def mutate_string_list(seed: list, pos: int = None, vocab: int = None):\n",
    "    if vocab is None:\n",
    "        vocab_str = random.sample(all_vocab, 1)[0]\n",
    "        vocab = word2ind[vocab_str]\n",
    "    if pos is None:\n",
    "        pos = random.randint(0, MAX_LENGTH-2) # MAX_LENGTH-2 inclusive\n",
    "    if pos != MAX_LENGTH-1: # should never replace EOS\n",
    "        seed[pos] = vocab\n",
    "    return eos_and_ind(seed, ind=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "outside-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLFuzz:\n",
    "    def __init__(self, rewarding=None):\n",
    "        if rewarding is None:\n",
    "            self.init_string = init_string_list() # always as index\n",
    "        else:\n",
    "            self.init_string = rewarding.copy()\n",
    "        self.seed_str = self.init_string.copy()\n",
    "        self.last_str = self.init_string.copy()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\\nOrg: \\n{\" \".join([ind2word[s] for s in self.init_string])}\\n Current seed string: \\n{\" \".join([ind2word[s] for s in self.seed_str])}'\n",
    "\n",
    "    def action(self, pos, vocab):\n",
    "        self.last_str = self.seed_str.copy()\n",
    "        self.seed_str = mutate_string_list(seed=self.seed_str, pos=pos, vocab=vocab)\n",
    "\n",
    "class RLFuzzEnv:\n",
    "    MUTATION_PENALTY = 1\n",
    "    SAME_STRING_PENALTY = 10 # eos & grammar related\n",
    "    PARSER_PENALTY = 20\n",
    "    SUCCESS_REWARD = 1000\n",
    "    ACTION_SPACE_SIZE_POS = MAX_LENGTH\n",
    "    ACTION_SPACE_SIZE_VOCAB = VOCAB_SIZE\n",
    "\n",
    "    def reset(self, rewarding=None):\n",
    "        self.session = StubSession()\n",
    "        self.last_status = 0\n",
    "        self.fuzzer = RLFuzz(rewarding)\n",
    "        self.episode_step = 0\n",
    "        observation = self.fuzzer.init_string\n",
    "        \n",
    "        self.gmod = GCheckModifier()\n",
    "        self.gparse = parser()\n",
    "        \n",
    "        return observation\n",
    "\n",
    "    def step(self, action_pos, action_vocab):       \n",
    "        fuzzing_success = False # init  \n",
    "        self.episode_step += 1\n",
    "        self.fuzzer.action(action_pos, action_vocab)\n",
    "        new_observation = self.fuzzer.seed_str\n",
    "        \n",
    "        eos_index = new_observation.index(0)\n",
    "        new_observation_ = new_observation[:eos_index]\n",
    "        username_rl = \" \".join([ind2word[s] for s in new_observation_])\n",
    "                \n",
    "        eos_indexL = self.fuzzer.last_str.index(0)\n",
    "        new_observationL_ = self.fuzzer.last_str[:eos_indexL]\n",
    "        last_username_rl = \" \".join([ind2word[s] for s in new_observationL_])\n",
    "                \n",
    "        parser_failed = self.gparse.main(self.gmod.grammarchecker(username_rl))\n",
    "        \n",
    "        if len(username_rl.strip()) == 0 or last_username_rl==username_rl: # rudimentary\n",
    "#             print(f\"SAME_STRING_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "            reward = -self.SAME_STRING_PENALTY\n",
    "        elif parser_failed: # parser\n",
    "#             print(f\"PARSER_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "            reward = -self.PARSER_PENALTY\n",
    "        else: # check via website\n",
    "            if self.last_status == 1:\n",
    "                self.session.reset_session()\n",
    "\n",
    "            url=\"http://localhost/demo/example_mysql_injection_login.php\"\n",
    "            jsonFilePath = './Stub/conditions.json'\n",
    "            receive=self.session.s.get(url)\n",
    "            form_details,keys=self.session.preprocessing_Form_Fields(url)\n",
    "\n",
    "            values=[username_rl, \"RaNdOmStRiNg\"]\n",
    "            logindata=self.session.form_input_feeding(keys,values,form_details)\n",
    "            pass_Conditions, fail_Conditions = self.session.jsonReading(jsonFilePath)\n",
    "            status = self.session.validation(url, logindata, keys, pass_Conditions, fail_Conditions)\n",
    "            self.last_status = status\n",
    "\n",
    "            fuzzing_success = True if status==1 else False\n",
    "\n",
    "            if fuzzing_success:\n",
    "                print(f\"SUCCESS_REWARD @ {self.episode_step}: \", username_rl)\n",
    "                reward = self.SUCCESS_REWARD\n",
    "            else:\n",
    "    #             print(f\"MUTATION_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "                reward = -self.MUTATION_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if self.episode_step >= 100 or (fuzzing_success and self.episode_step>10): #TODO: chk -- removed: @ SUCCESS_REWARD\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output, weights = self.att(inputs, inputs, return_attention_scores=True)\n",
    "        attn_output1 = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output1)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output), weights\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "# Agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "        #self.tensorboard = ModifiedTensorBoard(log_dir=\"logs\\\\{}-{}\".format(MODEL_NAME, int(time.time())), profile_batch = 10000000)\n",
    "        #$REM\n",
    "        \n",
    "    def create_model(self):\n",
    "        embed_dim = 64  # Embedding size for each token\n",
    "        num_heads = 4  # Number of attention heads\n",
    "        ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "        inputs = Input(shape=(MAX_LENGTH,))\n",
    "        embedding_layer = TokenAndPositionEmbedding(MAX_LENGTH, VOCAB_SIZE, embed_dim)\n",
    "        x = embedding_layer(inputs)\n",
    "        transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x, attn_output = transformer_block(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "\n",
    "        x_pos = layers.Dense(32, activation=\"relu\")(x)\n",
    "        x_pos = layers.Dropout(0.1)(x_pos)\n",
    "        x_pos = layers.Dense(env.ACTION_SPACE_SIZE_POS, activation='linear', name='q_pos')(x_pos)\n",
    "\n",
    "        x_vocab = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x_vocab = layers.Dropout(0.1)(x_vocab)\n",
    "        x_vocab = layers.Dense(env.ACTION_SPACE_SIZE_VOCAB, activation='linear', name='q_vocab')(x_vocab)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=[x_pos, x_vocab])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=\"accuracy\")\n",
    "        return model\n",
    "\n",
    "    # (observation space, action_pos, action_vocab, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self, terminal_state, step):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        # print(current_qs_list[0].shape, current_qs_list[1].shape) # 64,11 & 64,60\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[-2] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X, y_pos, y_vocab = [], [], []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action_pos, action_vocab, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q_pos = np.max(future_qs_list[0][index])\n",
    "                new_q_pos = reward + DISCOUNT * max_future_q_pos\n",
    "                \n",
    "                max_future_q_vocab = np.max(future_qs_list[1][index])\n",
    "                new_q_vocab = reward + DISCOUNT * max_future_q_vocab\n",
    "            else:\n",
    "                new_q_pos = reward\n",
    "                new_q_vocab = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs_pos = current_qs_list[0][index]\n",
    "            current_qs_pos[action_pos] = new_q_pos\n",
    "            \n",
    "            current_qs_vocab = current_qs_list[1][index]\n",
    "            current_qs_vocab[action_vocab] = new_q_vocab\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y_pos.append(current_qs_pos)\n",
    "            y_vocab.append(current_qs_vocab)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X), [np.array(y_pos), np.array(y_vocab)], batch_size=MINIBATCH_SIZE, verbose=0, \n",
    "                       shuffle=False if terminal_state else None)\n",
    "        #$REM: ,callbacks=[self.tensorboard] \n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.expand_dims(np.array(state), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incredible-submission",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|2                                                                        | 3/1000 [00:37<3:00:42, 10.87s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS_REWARD @ 1:  ' or ' 1'= ' 1 ' -- --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|4                                                                        | 6/1000 [01:36<4:40:17, 16.92s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS_REWARD @ 2:  ' or 1=1 ; / [ )\n",
      "SUCCESS_REWARD @ 4:  ' or 1=1 ; / [ ) instance/\n",
      "SUCCESS_REWARD @ 5:  ' or 1=1 ; 3 [ ) instance/\n",
      "SUCCESS_REWARD @ 6:  ' or 1=1 ; password [ ) instance/\n",
      "SUCCESS_REWARD @ 7:  ' or 1=1 ; password [ ) 1=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|##                                                                      | 29/1000 [09:46<5:27:14, 20.22s/episodes]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c3bdf248b134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# At every step update replay memory and train main network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_replay_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-6a7fda4d6a9d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, terminal_state, step)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;31m# When using target network, query it, otherwise main network should be queried\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mnew_current_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtransition\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mfuture_qs_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_current_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = RLFuzzEnv()\n",
    "ep_rewards = [-200]\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode #$REM\n",
    "    # agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward, step number, and env + get current state\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "#     if episode == 2:\n",
    "#         current_state = env.reset(succ)\n",
    "#     else:\n",
    "    current_state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            q_values = agent.get_qs(current_state)\n",
    "            action_pos = np.argmax(q_values[0][0])\n",
    "            action_vocab = np.argmax(q_values[1][0])\n",
    "            # action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            action_pos = np.random.randint(0, env.ACTION_SPACE_SIZE_POS-1) # randint a<=N<=b\n",
    "            action_vocab = np.random.randint(0, env.ACTION_SPACE_SIZE_VOCAB-1)\n",
    "\n",
    "        new_state, reward, done = env.step(action_pos, action_vocab)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # At every step update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action_pos, action_vocab, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Logging\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        #$REM agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "        if min_reward > MIN_REWARD:\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-duplicate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "#     # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.step = 1\n",
    "#         self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "#         self._log_write_dir = self.log_dir\n",
    "\n",
    "#     # Overriding this method to stop creating default log writer\n",
    "#     def set_model(self, model):\n",
    "#         pass\n",
    "\n",
    "#     # Overrided, saves logs with our step number\n",
    "#     # (otherwise every .fit() will start writing from 0th step)\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         self.update_stats(**logs)\n",
    "\n",
    "#     # Overrided\n",
    "#     # We train for one batch only, no need to save anything at epoch end\n",
    "#     def on_batch_end(self, batch, logs=None):\n",
    "#         pass\n",
    "\n",
    "#     # Overrided, so won't close writer\n",
    "#     def on_train_end(self, _):\n",
    "#         pass\n",
    "\n",
    "#     # Custom method for saving own metrics\n",
    "#     # Creates writer, writes custom metrics and closes writer\n",
    "#     def update_stats(self, **stats):\n",
    "#         self._write_logs(stats, self.step)\n",
    "        \n",
    "#     def _write_logs(self, logs, index):\n",
    "#         with self.writer.as_default():\n",
    "#             for name, value in logs.items():\n",
    "#                 tf.summary.scalar(name, value, step=index)\n",
    "#                 self.step += 1\n",
    "#                 self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerBlock(layers.Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "#         self.ffn = keras.Sequential(\n",
    "#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "#         )\n",
    "#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout1 = layers.Dropout(rate)\n",
    "#         self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "#     def call(self, inputs, training):\n",
    "#         attn_output = self.att(inputs, inputs)\n",
    "#         attn_output1 = self.dropout1(attn_output, training=training)\n",
    "#         out1 = self.layernorm1(inputs + attn_output1)\n",
    "#         ffn_output = self.ffn(out1)\n",
    "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
    "#         return self.layernorm2(out1 + ffn_output), attn_output\n",
    "    \n",
    "# class TokenAndPositionEmbedding(layers.Layer):\n",
    "#     def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "#         super(TokenAndPositionEmbedding, self).__init__()\n",
    "#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "#         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         maxlen = tf.shape(x)[-1]\n",
    "#         positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "#         positions = self.pos_emb(positions)\n",
    "#         x = self.token_emb(x)\n",
    "#         return x + positions\n",
    "\n",
    "# embed_dim = 32  # Embedding size for each token\n",
    "# num_heads = 2  # Number of attention heads\n",
    "# ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# inputs = Input(shape=(MAX_LENGTH,))\n",
    "# embedding_layer = TokenAndPositionEmbedding(MAX_LENGTH, VOCAB_SIZE, embed_dim)\n",
    "# x = embedding_layer(inputs)\n",
    "# transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "# x, attn_output = transformer_block(x)\n",
    "# x = GlobalAveragePooling1D()(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = Dense(64, activation=\"relu\")(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "\n",
    "# x_pos = Dense(32, activation=\"relu\")(x)\n",
    "# x_pos = Dropout(0.1)(x_pos)\n",
    "# x_pos = Dense(env.ACTION_SPACE_SIZE_POS, activation='linear')(x_pos)\n",
    "\n",
    "# x_vocab = Dense(32, activation=\"relu\")(x)\n",
    "# x_vocab = Dropout(0.1)(x_vocab)\n",
    "# x_vocab = Dense(env.ACTION_SPACE_SIZE_VOCAB, activation='linear')(x_vocab)\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=[x_pos, x_vocab])\n",
    "\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
