{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchbar penalty + login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import scipy.signal\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Stub.Stub import StubSession\n",
    "from grammar_lib.testCaseModifier import GCheckModifier\n",
    "from grammar_lib.SQLChecker import parser\n",
    "\n",
    "# with open(\"grammar_lib/all_grammar_inputs.txt\") as file:\n",
    "#     all_grammar_inputs = [line.strip() for line in file]\n",
    "\n",
    "all_grammar_inputs = [\"' ) UNION select NULL,email,pass,NULL from user;\",\n",
    "                        \"' ) UNION select NULL,email,pass,NULL from user;\",\n",
    "                        \"' null UNION select NULL,email,pass,NULL from user;\",\n",
    "                        \"' null UNION select NULL,email,pass,NULL from user;\",\n",
    "                        \"' ) UNION select NULL,email,pass,NULL from user\",\n",
    "                        \"' ) UNION select NULL,email,pass,NULL from user\"]\n",
    "    \n",
    "all_grammar_inputs_mod = []\n",
    "for gram_inp in all_grammar_inputs:\n",
    "    all_grammar_inputs_mod.append(gram_inp)\n",
    "    all_grammar_inputs_mod.append(gram_inp.replace(\"'\", ')'))\n",
    "\n",
    "all_grammar_inputs1 = all_grammar_inputs_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grammar_lib/all_grammar_inputs.txt\") as file:\n",
    "    all_grammar_inputs = [line.strip() for line in file]\n",
    "\n",
    "all_grammar_inputs_mod = []\n",
    "for gram_inp in all_grammar_inputs:\n",
    "    if not any(txt_chk in gram_inp.lower() for txt_chk in ['union', 'order', 'admin', '/*', \"1'=\"]):\n",
    "        all_grammar_inputs_mod.append(gram_inp)\n",
    "        all_grammar_inputs_mod.append(gram_inp.replace('OR', 'AND'))\n",
    "        if random.random()<0.5:\n",
    "            all_grammar_inputs_mod.append(gram_inp.replace(\"'\", '('))\n",
    "\n",
    "all_grammar_inputs2 = all_grammar_inputs_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grammar_inputs = all_grammar_inputs1+all_grammar_inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = all_grammar_inputs.copy()\n",
    "\n",
    "sent_processed = []\n",
    "tokenizer = WordPunctTokenizer()\n",
    "max_len = 0\n",
    "\n",
    "for input_str in sent:\n",
    "    input_str = str(input_str)\n",
    "    input_str = input_str.lower()\n",
    "    input_str = input_str.strip()\n",
    "    str_list = nltk.word_tokenize(input_str)\n",
    "    if len(str_list) > max_len:\n",
    "        max_len = len(str_list)\n",
    "    sent_processed.append(str_list)\n",
    "    # sent_processed.append(tokenizer.tokenize(input_str))\n",
    "    \n",
    "all_vocab = [v_w for v in sent_processed for v_w in v]\n",
    "\n",
    "v_count = dict(Counter(all_vocab))\n",
    "v_count = dict(sorted(v_count.items(), key=lambda item: item[1], reverse=True))\n",
    "all_vocab = list(v_count.keys())\n",
    "\n",
    "all_vocab.sort()\n",
    "\n",
    "ind_list = list(range(1, len(all_vocab)+1))\n",
    "\n",
    "word2ind = dict(zip(all_vocab, ind_list))\n",
    "ind2word = dict(zip(ind_list, all_vocab))\n",
    "\n",
    "word2ind['<EOS>'] = 0\n",
    "ind2word[0] = '<EOS>'\n",
    "\n",
    "all_vocab = ['<EOS>']+all_vocab\n",
    "\n",
    "VOCAB_SIZE = len(all_vocab)\n",
    "\n",
    "VOCAB_SIZE, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters of the PPO algorithm\n",
    "MODEL_NAME = \"RLFuzzPPOCombined_v1\"\n",
    "steps_per_epoch = 50\n",
    "epochs = 5000\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15 # including EOS\n",
    "\n",
    "loaded_test = [] # store compatible length grammar based init (generation) test strings\n",
    "for sent in sent_processed:\n",
    "    if len(sent)<=MAX_LENGTH-1:\n",
    "        loaded_test.append(sent)\n",
    "        \n",
    "len(loaded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eos_and_ind(sampled_list: list, ind: bool = False):\n",
    "    if ind:\n",
    "        eos_token = 0\n",
    "    else:\n",
    "        eos_token = \"<EOS>\"\n",
    "    clip_ind = sampled_list.index(eos_token)\n",
    "    # clip_ind = np.where(sampled_list==eos_token)[0][0] # for numpy\n",
    "    if clip_ind < MAX_LENGTH-1:\n",
    "        clip_ind_rem = MAX_LENGTH-clip_ind\n",
    "        sampled_list = sampled_list[:clip_ind]+[eos_token]*clip_ind_rem # slower than if\n",
    "    assert len(sampled_list) == MAX_LENGTH\n",
    "    if ind:\n",
    "        return sampled_list\n",
    "    return [word2ind[s] for s in sampled_list]\n",
    "\n",
    "def init_string_list():\n",
    "    gram_gen_str = loaded_test[random.randint(0, len(loaded_test)-1)] # randint a<=N<=b\n",
    "    sampled_list = gram_gen_str+(MAX_LENGTH-len(gram_gen_str))*['<EOS>']\n",
    "    # sampled_list = list(random.sample(all_vocab, MAX_LENGTH-1))+['<EOS>']\n",
    "    return eos_and_ind(sampled_list)\n",
    "\n",
    "def mutate_string_list(seed: list, pos: int = None, vocab: int = None):\n",
    "    if vocab is None:\n",
    "        vocab_str = random.sample(all_vocab, 1)[0]\n",
    "        vocab = word2ind[vocab_str]\n",
    "    if pos is None:\n",
    "        pos = random.randint(0, MAX_LENGTH-2) # MAX_LENGTH-2 inclusive\n",
    "    if pos != MAX_LENGTH-1: # should never replace EOS\n",
    "        seed[pos] = vocab\n",
    "    return eos_and_ind(seed, ind=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLFuzz:\n",
    "    def __init__(self, rewarding: list = None):\n",
    "        if rewarding is None:\n",
    "            self.init_string = init_string_list() # always as index\n",
    "        else:\n",
    "            self.init_string = rewarding.copy()\n",
    "        self.seed_str = self.init_string.copy()\n",
    "        self.last_str = self.init_string.copy()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\\nOrig seed string: \\n{\" \".join([ind2word[s] for s in self.init_string])}\\nLast seed string: \\n{\" \".join([ind2word[s] for s in self.last_str])}\\nCurr seed string: \\n{\" \".join([ind2word[s] for s in self.seed_str])}'\n",
    "\n",
    "    def action(self, pos, vocab):\n",
    "        self.last_str = self.seed_str.copy()\n",
    "        self.seed_str = mutate_string_list(seed=self.seed_str, pos=pos, vocab=vocab)\n",
    "\n",
    "class RLFuzzEnv:\n",
    "    EXCEPTION_PENALTY = 0.1\n",
    "    MUTATION_PENALTY = 0.2\n",
    "    SAME_STRING_PENALTY = 0.3 # eos & grammar related\n",
    "    PARSER_PENALTY = 0.5\n",
    "    SUCCESS_REWARD = 5\n",
    "    ACTION_SPACE_SIZE_POS = MAX_LENGTH\n",
    "    ACTION_SPACE_SIZE_VOCAB = VOCAB_SIZE\n",
    "\n",
    "    def reset(self, rewarding=None):\n",
    "        self.session = StubSession()\n",
    "        self.last_status = 0\n",
    "        self.fuzzer = RLFuzz(rewarding)\n",
    "        self.episode_step = 0\n",
    "        observation = self.fuzzer.init_string\n",
    "        \n",
    "        self.gmod = GCheckModifier()\n",
    "        self.gparse = parser()\n",
    "        \n",
    "        self.operation = 'None'\n",
    "        \n",
    "        return np.array(observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        action_pos, action_vocab = self.breakdown_action(action)\n",
    "        fuzzing_success = False # init  \n",
    "        self.episode_step += 1\n",
    "        self.fuzzer.action(action_pos, action_vocab)\n",
    "        new_observation = self.fuzzer.seed_str\n",
    "        \n",
    "        eos_index = new_observation.index(0)\n",
    "        new_observation_ = new_observation[:eos_index]\n",
    "        username_rl = \" \".join([ind2word[s] for s in new_observation_])\n",
    "                \n",
    "        eos_indexL = self.fuzzer.last_str.index(0)\n",
    "        new_observationL_ = self.fuzzer.last_str[:eos_indexL]\n",
    "        last_username_rl = \" \".join([ind2word[s] for s in new_observationL_])\n",
    "                \n",
    "        parser_failed = self.gparse.main(self.gmod.grammarchecker(username_rl))\n",
    "        \n",
    "        if self.episode_step>1 and (len(username_rl.strip()) == 0 or last_username_rl==username_rl): # rudimentary\n",
    "            # print(f\"SAME_STRING_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "            reward = -self.SAME_STRING_PENALTY\n",
    "        elif parser_failed: # parser\n",
    "            # print(f\"PARSER_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "            reward = -self.PARSER_PENALTY\n",
    "        else: # check via website\n",
    "            if self.last_status == 1:\n",
    "                self.session.reset_session()\n",
    "            \n",
    "            # Search box\n",
    "\n",
    "            url=\"http://localhost/demo/example_mysql_injection_search_box.php\"\n",
    "            jsonFilePath = './Stub/conditions1.json'\n",
    "            receive=self.session.s.get(url)\n",
    "            form_details,keys=self.session.preprocessing_Form_Fields(url)\n",
    "\n",
    "            values=[username_rl]\n",
    "            logindata=self.session.form_input_feeding(keys,values,form_details)\n",
    "            pass_Conditions, fail_Conditions = self.session.jsonReading(jsonFilePath)\n",
    "            status = self.session.validation(url, logindata, keys, pass_Conditions, fail_Conditions)\n",
    "            \n",
    "            status_ex = self.session.exceptionCatcher(url, logindata)\n",
    "            \n",
    "            exception_success = True if status_ex==1 else False\n",
    "            \n",
    "            fuzzing_success1 = True if status==1 else False\n",
    "            \n",
    "            # Login page\n",
    "            \n",
    "            url=\"http://localhost/demo/example_mysql_injection_login.php\"\n",
    "            jsonFilePath = './Stub/conditions.json'\n",
    "            receive=self.session.s.get(url)\n",
    "            form_details,keys=self.session.preprocessing_Form_Fields(url)\n",
    "\n",
    "            values=[username_rl, \"RaNdOmStRiNg\"]\n",
    "            logindata=self.session.form_input_feeding(keys,values,form_details)\n",
    "            pass_Conditions, fail_Conditions = self.session.jsonReading(jsonFilePath)\n",
    "            status = self.session.validation(url, logindata, keys, pass_Conditions, fail_Conditions)\n",
    "            self.last_status = status\n",
    "            fuzzing_success2 = True if status==1 else False\n",
    "            \n",
    "            # Common\n",
    "\n",
    "            fuzzing_success = fuzzing_success1 or fuzzing_success2\n",
    "            \n",
    "            if fuzzing_success:\n",
    "#                 print(f\"\\nSUCCESS_REWARD @ {self.episode_step}: \")\n",
    "#                 print(self.fuzzer)\n",
    "#                 print(f\"pos: {action_pos}, vocab: {action_vocab} -> {ind2word[action_vocab]}\")\n",
    "                reward = self.SUCCESS_REWARD\n",
    "            elif exception_success:\n",
    "#                 print(f\"EXCEPTION_REWARD @ {self.episode_step}: \", username_rl)\n",
    "                reward = -self.EXCEPTION_PENALTY\n",
    "            else:\n",
    "                # print(f\"MUTATION_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "                reward = -self.MUTATION_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if self.episode_step >= steps_per_epoch or (fuzzing_success and self.episode_step>steps_per_epoch//4): #TODO: chk -- removed: @ SUCCESS_REWARD\n",
    "            done = True\n",
    "\n",
    "        return np.array(new_observation), reward, done\n",
    "    \n",
    "    def breakdown_action(self, action):\n",
    "        action_pos = action//VOCAB_SIZE\n",
    "        action_vocab = action%VOCAB_SIZE\n",
    "        # print('POS: ', action_pos, 'VOCAB: ', action_vocab, 'CONV: ', action_pos*VOCAB_SIZE+action_vocab)\n",
    "        return action_pos, action_vocab\n",
    "\n",
    "    def squeeze_actions(self, action_pos, action_vocab):\n",
    "        return action_pos*VOCAB_SIZE+action_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    x = layers.Embedding(VOCAB_SIZE, 16)(x)\n",
    "    x= layers.GRU(32, return_sequences=False)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-insert",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Initializations\n",
    "\"\"\"\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "agg_rewards = []\n",
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = RLFuzzEnv()\n",
    "observation_dimensions = MAX_LENGTH\n",
    "num_actions = MAX_LENGTH*VOCAB_SIZE\n",
    "print('num_actions: ', num_actions, f'{MAX_LENGTH}*{VOCAB_SIZE}')\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "\"\"\"\n",
    "## Train\n",
    "\"\"\"\n",
    "# Iterate over the number of epochs\n",
    "for epoch in tqdm(range(epochs), ascii=True, unit='episodes'):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    \n",
    "    success_count = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        \n",
    "        obs_text_list = [ind2word[i] for i in observation[0]]\n",
    "        \n",
    "        if \"and\" in obs_text_list and random.random()<=0.9:\n",
    "            # print(\"--- and->or ---@\", t)\n",
    "            action = [env.squeeze_actions(action_pos = obs_text_list.index(\"and\"), action_vocab = word2ind[\"or\"])]\n",
    "            action = tf.constant(action, dtype=tf.int64)\n",
    "        \n",
    "        elif \";\" not in obs_text_list and obs_text_list.index(\"<EOS>\")!=MAX_LENGTH-1 and random.random()<=0.9:\n",
    "            # print(\"--- semicolon ---@\", t)\n",
    "            action = [env.squeeze_actions(action_pos = obs_text_list.index(\"<EOS>\"), action_vocab = word2ind[\";\"])]\n",
    "            action = tf.constant(action, dtype=tf.int64)\n",
    "        \n",
    "        elif \"'\" in obs_text_list and \")\" not in obs_text_list and random.random()<=0.9:\n",
    "            # print(\"--- ') ---@\", t)\n",
    "            action = [env.squeeze_actions(action_pos = obs_text_list.index(\"'\")+1, action_vocab = word2ind[\")\"])]\n",
    "            action = tf.constant(action, dtype=tf.int64)\n",
    "            \n",
    "        elif \"'\" not in obs_text_list and random.random()<=0.9:\n",
    "            # print(\"--- apos --- @\", t)\n",
    "            action = [env.squeeze_actions(action_pos = 0, action_vocab = word2ind[\"'\"])]\n",
    "            action = tf.constant(action, dtype=tf.int64)\n",
    "            \n",
    "        observation_new, reward, done = env.step(action[0].numpy())\n",
    "        if reward > 0: success_count+=1\n",
    "#         print(\" \".join(obs_text_list))\n",
    "#         print(\" \".join(ind2word[i] for i in observation_new))\n",
    "#         print()\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    agg_rewards.append(round((sum_return / num_episodes), 2))\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {(sum_return / num_episodes):.2f}. Mean Length: {(sum_length / num_episodes):.2f}. Success Count: {success_count}\"\n",
    "    )\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        mean_agg = sum(agg_rewards[-100:])/len(agg_rewards[-100:])\n",
    "        print(\"Mean Aggregate: \", mean_agg)\n",
    "        actor.save(f'models/{MODEL_NAME}__ep_{epoch}__{(sum_return / num_episodes):.2f}__actor.h5')\n",
    "        critic.save(f'models/{MODEL_NAME}__ep_{epoch}__{(sum_return / num_episodes):.2f}__critic.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.save(f'models/{MODEL_NAME}_FINAL__ep_{epoch}__{(sum_return / num_episodes):.2f}__actor.h5')\n",
    "critic.save(f'models/{MODEL_NAME}_FINAL__ep_{epoch}__{(sum_return / num_episodes):.2f}__critic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-transformation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
